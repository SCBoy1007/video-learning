data:
  # Multi-dataset training: 5 train + 2 validation
  train_files: None  # Set in shell script
  val_files: None    # Set in shell script
  prompt_key: problem
  max_prompt_length: 1300  # Standard image prompt length
  max_response_length: 2000
  rollout_batch_size: 32  # Double batch size for faster training
  shuffle: true
  seed: 42
  max_pixels: 12845056  # 280×280 images fit comfortably
  min_pixels: 3136

algorithm:
  adv_estimator: grpo
  kl_coef: 0.0

worker:
  actor:
    global_batch_size: 32  # Double batch size for faster training
    micro_batch_size_per_device_for_update: 4  # Increase per-device batch
    micro_batch_size_per_device_for_experience: 4
    max_grad_norm: 1.0
    use_kl_loss: true
    kl_loss_coef: 5.0e-3
    kl_loss_type: low_var_kl
    model:
      model_path: Qwen/Qwen2.5-VL-7B-Instruct  # Overridden in shell
      enable_gradient_checkpointing: true
    optim:
      lr: 1.0e-6
      weight_decay: 1.0e-2
    fsdp:
      param_offload: false
      optimizer_offload: false
      torch_dtype: "bf16"  # BF16 for Flash Attention 2.0
    offload:
      param_offload: true
      optimizer_offload: true

  rollout:
    temperature: 1.0
    tensor_parallel_size: 2  # 2-way TP for 4×80G setup
    gpu_memory_utilization: 0.4  # Conservative memory usage
    n: 8  # 8 GRPO samples per prompt
    enable_chunked_prefill: false

  ref:
    offload:
      param_offload: true

  reward:
    reward_type: function
    compute_score: vision_reasoner  # Use original Seg-Zero reward

trainer:
  total_episodes: 15  # 15 episodes for proper training
  logger: ["console", "wandb"]
  project_name: brain_tumor_image_4x80G
  n_gpus_per_node: 4
  nnodes: 1
  save_freq: 50  # Save every 50 steps
  test_freq: 100  # Validate every 100 steps
  val_before_train: true
  val_only: false
  remove_previous_ckpt: false  # Keep all checkpoints (set true to save space)
