data:
  train_files: None  # ⚠️ OVERRIDDEN by run_brain_tumor_3d_4x80G.sh
  val_files: None    # ⚠️ OVERRIDDEN by run_brain_tumor_3d_4x80G.sh
  prompt_key: problem
  max_prompt_length: 28000  # Increased to accommodate video tokens (155 frames × ~130 tokens/frame ≈ 20k + text)
  max_response_length: 1200  # Balanced: detailed thinking (300+ words) without OOM
  rollout_batch_size: 16  # Aligned with Seg-Zero configuration
  shuffle: true
  seed: 42
  max_pixels: 12845056
  min_pixels: 3136

algorithm:
  adv_estimator: grpo
  kl_coef: 0.0  # KL penalty in reward calculation (0.0 = disabled, use kl_loss_coef instead)

worker:
  actor:
    global_batch_size: 16  # Increased to 16 to align with Seg-Zero
    micro_batch_size_per_device_for_update: 2  # Updated to match shell script
    micro_batch_size_per_device_for_experience: 2  # Updated to match shell script
    max_grad_norm: 1.0
    use_kl_loss: true
    kl_loss_coef: 5.0e-3  # Aligned with Seg-Zero (more conservative KL constraint)
    kl_loss_type: low_var_kl
    model:
      model_path: Qwen/Qwen2.5-VL-7B-Instruct  # ⚠️ Overridden by .sh to use local cache
      enable_gradient_checkpointing: true
    optim:
      lr: 1.0e-6  # Aligned with Seg-Zero (10x lower for stability)
      weight_decay: 1.0e-2
      lr_warmup_steps_ratio: 0.0  # No warmup
      training_steps: 1725  # ~115 batches × 15 episodes (batch_size=16)
    fsdp:
      param_offload: false
      optimizer_offload: false
      torch_dtype: null
    offload:
      param_offload: true
      optimizer_offload: true

  rollout:
    temperature: 1.0
    tensor_parallel_size: 1  # Single GPU inference (7B model fits easily)
    gpu_memory_utilization: 0.55  # Further reduced from 0.6 to leave more room for FSDP
    n: 8  # Increased to 8 for better GRPO gradient estimation and exploration
    enable_chunked_prefill: false  # Disabled to avoid vLLM compatibility issues
    max_num_batched_tokens: 32768  # Increased to handle video multi-modal embeddings

  ref:
    offload:
      param_offload: true

  reward:
    reward_type: function
    compute_score: brain_tumor_3d

trainer:
  total_episodes: 15  # Increased for proper RL training
  logger: ["console", "wandb"]  # Enable WandB for score tracking
  project_name: brain_tumor_3d_4x80G
  n_gpus_per_node: 4  # 4 GPUs per node
  nnodes: 1
  save_freq: 10
  test_freq: 230  # Validate every 2 episodes (approx. 115 steps/episode × 2)
  val_before_train: true  # Validate before training starts
  val_only: false